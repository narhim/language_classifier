\documentclass[11pt,a4paper]{article}
\usepackage{times}
\usepackage{latexsym}
%Tables package.
%\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage[shortlabels]{enumitem}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
%Examples package
\usepackage{gb4e}
%The general problem area
%The particular question addressed
%The way you address it
%The findings
%The significance of the findings
%Next steps to be taken

\title{Review of Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank \citep{socher2013recursive}}

\date{}

\author{Teresa Mart√≠n Soeder}
\begin{document}
\maketitle

This paper introduces the Stanford Sentiment Treebank with sentiment analysis annotations and the Recursive Neural Tensor Network (RNTN), designed to predict the compositional semantic effects present in the corpus. Furthermore, they provide evidence that shows that the RNTN has greater accuracy than a standard Recursive Neural Network (RNN) and a Matrix-Vector RNN (MV-RNN) when classifying the sentences in the corpus into a fine-grained classification of 5 labels, into a binary classification, in subsets consisting of positive or negative sentences and their negation, and in a subset for the analysis of the conjunction \textit{but}.\\

The corpus, which consists of phrases extracted from movie reviews, was parsed with the Stanford Parsed and annotated by 3 turkers on a 25-point scale. Then, based on the annotators' use of the labels, these 25 points were reduced to 5 labels for the experiments. This is partially reminiscent of more recent studies like that of \citet{pavlick2019inherent}, but it does not hold the test of time since the number of annotators is minimal and there is no consideration of possible inherent disagreement, a phenomenon very common in sentiment analysis \citep{uma2021learning}, or even an inter-annotator agreement score. As to the mapping from 25 to 5 labels, it seems plausible given the graphics in Figure 2, but, since for full sentences there appears to be a more even distribution along the whole scale, it would have been good to see a discussion of the limitations of this set of labels for full sentences and longer texts. Furthermore, a graphic clearly showing the mapping from 25 to 5 labels and a clarification on how many labels the available corpus has, would have been helpful.\\

Regarding the RNTN, the detailed explanation about how the computations are made and the comparisons to its closed relatives RNN and MV-RNN is really good and helps in understanding the value of the RNTN. Furthermore, the consideration of cognitive and linguistic plausibility by implementing an efficient method for semantic compositionality is very interesting and one wonders what Natural Language Explanation Techniques would reflect if applied to each model. In regards to the experiments, they seemed to support the combination of linguistic and cognitive plausibility characteristic of the RNTN, but given that they only provide accuracy as an evaluation metric and that we ignore the exact corpus distribution among the 5 labels, it appears that more evidence is needed to certify such improvement. Lastly, it would have been good to provide a discussion of the limits of the model especially since semantic compositionality has long been known to have problems with, for example, idioms like \textit{At the end, the movie went down in flames}. 

In conclusion, it is possible to say that this study holds to the standards of the time it was published, but it is short on information on both of the corpus and the evaluation metrics.

\nocite{*}
\bibliography{references}
\bibliographystyle{chicago}
\end{document}
